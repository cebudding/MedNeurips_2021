{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data with MRI background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that data generation is slightly different than in the other notebooks; this is an older version. However, as the MRI data is not available anymore in the preprocessed form, the updated version could not be tested with this background. \n",
    "\n",
    "It would be strongly recommended to generate the MRI data with the other notebook or to harmonize the two implementations. One simple way to do this is simply generating a numpy array with all MRI slices and passing it as a background in the other notebook and simply treating is as a noise condition, as also described in the manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: since the MRI data is privacy-sensitive, the outputs have been cleared from this notebook. The data is from the Human Connectome project and has been preprocessed according to the manuscript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('image', cmap='gray')\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time, datetime \n",
    "\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/ritter/share/data/Connectome/connectome_BIDS_processed/'\n",
    "data_dir = '/ritter/share/projects/Methods/Budding_interpret_ground_truth/data/hamming_single_lesion_lateral/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(working_dir, 'participants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parts = pd.read_csv(path)\n",
    "data_parts['subj_idx'] = np.arange(len(data_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = data_parts[data_parts['split'] == 'train']\n",
    "validation_set = data_parts[data_parts['split'] == 'val']\n",
    "holdout_set = data_parts[data_parts['split'] == 'holdout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for lesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(working_dir, training_set['path'][407])\n",
    "scan = nib.load(path_data)\n",
    "struct_arr = scan.get_data().astype(np.float32)\n",
    "struct_arr = struct_arr[10:150, :, :135]\n",
    "slices = [struct_arr[:, :, i] for i in range(struct_arr.shape[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 8))\n",
    "for i in range(10): \n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.imshow(slices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(struct_arr, num_lesions = 1, mask = None):\n",
    "    lesion_img = deepcopy(struct_arr)\n",
    "    \n",
    "    list_coords = []\n",
    "    \n",
    "    if mask is not None: \n",
    "        struct_arr = mask * struct_arr\n",
    "    \n",
    "    for i in range(num_lesions):\n",
    "        lesion = 'none'\n",
    "        while lesion == 'none': \n",
    "            coord_1 = np.random.randint(0, struct_arr.shape[0])\n",
    "            coord_2 = np.random.randint(0, struct_arr.shape[1])\n",
    "            if struct_arr[coord_1, coord_2] != 0: \n",
    "                lesion = 'success'\n",
    "                \n",
    "        list_coords.append((coord_1, coord_2))\n",
    "                \n",
    "    return list_coords\n",
    "\n",
    "def gaus2d(x=0, y=0, mx=0, my=0, sx=2, sy=2):\n",
    "    return 1. / (2. * np.pi * sx * sy) * np.exp(-((x - mx)**2. / (2. * sx**2.) + (y - my)**2. / (2. * sy**2.)))\n",
    "\n",
    "def _nd_window(data, filter_function, **kwargs):\n",
    "    \"\"\"\n",
    "    Performs an in-place windowing on N-dimensional spatial-domain data.\n",
    "    This is done to mitigate boundary effects in the FFT.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "           Input data to be windowed, modified in place.\n",
    "    filter_function : 1D window generation function\n",
    "           Function should accept one argument: the window length.\n",
    "           Example: scipy.signal.hamming\n",
    "    \"\"\"\n",
    "    for axis, axis_size in enumerate(data.shape):\n",
    "        # set up shape for numpy broadcasting\n",
    "        filter_shape = [1, ] * data.ndim\n",
    "        filter_shape[axis] = axis_size\n",
    "        window = filter_function(axis_size, **kwargs).reshape(filter_shape)\n",
    "        # scale the window intensities to maintain image intensity\n",
    "        np.power(window, (1.0/data.ndim), out=window)\n",
    "        data *= window\n",
    "        \n",
    "    return data\n",
    "\n",
    "def create_simple_lesions(struct_arr, list_coords, size, intensities = 0, shape = 'square', lesion_type = 'opaque', window_type = None): \n",
    "    lesion_img = deepcopy(struct_arr)\n",
    "    ground_truth = np.zeros((struct_arr.shape[0], struct_arr.shape[1]))\n",
    "    \n",
    "    for i, coord in enumerate(list_coords):        \n",
    "        if shape[i] == 'square':\n",
    "            intensity = intensities[i]\n",
    "            coords = [coord[0]- size, coord[0] + size, coord[1] - size, coord[1] + size]\n",
    "            corr_coord = [0 if i < 0 else i for i in coords]\n",
    "            \n",
    "            if lesion_type[i] == 'opaque':\n",
    "                lesion_img[corr_coord[0]:corr_coord[1], corr_coord[2]:corr_coord[3]] = intensity\n",
    "                \n",
    "            elif lesion_type[i] == 'reduced': \n",
    "                masked_area = struct_arr[corr_coord[0]:corr_coord[1], corr_coord[2]:corr_coord[3]]\n",
    "                lesion_img[corr_coord[0]:corr_coord[1], corr_coord[2]:corr_coord[3]] = intensity * masked_area\n",
    "                \n",
    "            ground_truth[corr_coord[0]:corr_coord[1], corr_coord[2]:corr_coord[3]] = 1\n",
    "                \n",
    "        elif shape[i] == 'circular':\n",
    "            intensity = intensities[i]\n",
    "            h = struct_arr.shape[0]\n",
    "            l = struct_arr.shape[1]\n",
    "            \n",
    "            y,x = np.ogrid[-coord[0]:h-coord[0], -coord[1]:l-coord[1]]\n",
    "            mask = x*x + y*y <= size*size\n",
    "\n",
    "            if lesion_type[i] == 'opaque': \n",
    "                new_value = intensity\n",
    "            elif lesion_type[i] == 'reduced': \n",
    "                masked_area = struct_arr[mask]\n",
    "                new_value = intensity * masked_area\n",
    "                \n",
    "            lesion_img[mask] = new_value\n",
    "            \n",
    "            ground_truth[mask] = 1\n",
    "            \n",
    "        elif shape[i] == 'gaussian': \n",
    "            x = np.linspace(0, struct_arr.shape[1], struct_arr.shape[1])\n",
    "            y = np.linspace(0, struct_arr.shape[0], struct_arr.shape[0])\n",
    "            x, y = np.meshgrid(x, y) # get 2D variables instead of 1D\n",
    "            z = gaus2d(x, y, mx = coord[1], my = coord[0], sx = size, sy = size)\n",
    "            z /= z.max()\n",
    "            \n",
    "            ground_truth += z\n",
    "            \n",
    "            z = (z * -1) + 1\n",
    "            z = minmax_scale(z.flatten(), feature_range = (intensities[i], 1)).reshape(struct_arr.shape[0], struct_arr.shape[1])\n",
    "            \n",
    "            lesion_img = z * lesion_img\n",
    "            \n",
    "        elif shape[i] == 'window': \n",
    "            window_fnc = window_type\n",
    "            \n",
    "            masked_area = np.ones((2*size, 2*size))\n",
    "            lesion = _nd_window(masked_area, filter_function = window_fnc)\n",
    "            padded_img = np.ones((struct_arr.shape[0] + 2 * size, struct_arr.shape[1] + 2 * size))\n",
    "                        \n",
    "            padded_img[coord[0]: coord[0] + 2 * size, coord[1]: coord[1] + 2 * size] = (lesion * - 1) + 1\n",
    "            img = padded_img[size:padded_img.shape[0] - size, size:padded_img.shape[1] - size]\n",
    "            \n",
    "            img = minmax_scale(img.flatten(), feature_range = (intensities[i], 1)).reshape(struct_arr.shape[0], struct_arr.shape[1])\n",
    "            \n",
    "            lesion_img *= img\n",
    "            ground_truth += (img * -1) + 1 \n",
    "    \n",
    "        else: \n",
    "            print(\"please provide lesion shape\")\n",
    "            \n",
    "        mask = struct_arr > 0 \n",
    "        lesion_img = lesion_img * mask\n",
    "        ground_truth = ground_truth * mask\n",
    "        \n",
    "    return lesion_img, ground_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nonlinear_lesions(working_dir, dataset, num_lesions = 1, size = 5, intensity = 0.5, shape = 'circular', lesion_type = 'reduced', frac_pos = 0.5, both_lesion = False, window_type = None): \n",
    "    lesions_parts = []\n",
    "    all_slices = []\n",
    "    all_ground_truth = []\n",
    "    all_labels = []\n",
    "\n",
    "    participant_list = dataset['subj_idx']\n",
    "\n",
    "    for part in participant_list: \n",
    "        print(part)\n",
    "\n",
    "        path_data = os.path.join(working_dir, dataset['path'][part])\n",
    "        scan = nib.load(path_data)\n",
    "        struct_arr = scan.get_data().astype(np.float32)\n",
    "        struct_arr = struct_arr[10:150, :, :135]\n",
    "        slices = [struct_arr[:, :, i] for i in range(struct_arr.shape[2])]\n",
    "\n",
    "        coords = []\n",
    "\n",
    "        labels = np.zeros(len(slices))\n",
    "        num_positives = int(frac_pos * len(slices))\n",
    "        indices = np.arange(len(slices))\n",
    "\n",
    "        both_lesions, separate_lesions = train_test_split(indices, test_size = 0.67)\n",
    "        lesion_1, lesion_2 = train_test_split(separate_lesions, test_size = frac_pos)\n",
    "\n",
    "        ground_truths = []\n",
    "\n",
    "        for i, img in enumerate(slices): \n",
    "            if i in both_lesions: \n",
    "                coords = get_coordinates(slices[i], num_lesions = 2)\n",
    "                lesion_img, ground_truth = create_simple_lesions(slices[i], list_coords = coords, size = size, \n",
    "                                                                intensities = intensities, shape = shape, lesion_type = lesion_type, window_type = window_type)\n",
    "\n",
    "                slices[i] = lesion_img\n",
    "                labels[i] = 1            \n",
    "            elif i in lesion_1: \n",
    "                shape_1 = [shape[0]]\n",
    "                coords = get_coordinates(slices[i], num_lesions = 1)\n",
    "                lesion_img, ground_truth = create_simple_lesions(slices[i], list_coords = coords, size = size, \n",
    "                                                                intensities = [intensities[0]], shape = shape_1, lesion_type = [lesion_type[0]], window_type = window_type)\n",
    "\n",
    "                slices[i] = lesion_img\n",
    "                labels[i] = 0\n",
    "            elif i in lesion_2:\n",
    "                shape_2 = [shape[1]]\n",
    "                coords = get_coordinates(slices[i], num_lesions = 1)\n",
    "                lesion_img, ground_truth = create_simple_lesions(slices[i], list_coords = coords, size = size, \n",
    "                                                                intensities = [intensities[1]], shape = shape_2, lesion_type = [lesion_type[1]], window_type = window_type)\n",
    "\n",
    "                slices[i] = lesion_img\n",
    "                labels[i] = 0\n",
    "\n",
    "            ground_truths.append((i, coords, ground_truth))\n",
    "\n",
    "        all_slices.append(slices)\n",
    "        all_ground_truth.append(ground_truths)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return lesions_parts, all_slices, all_ground_truth, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save datasets with one type of lesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lesions_slices(working_dir, dataset, num_lesions = 1, size = 5, intensity = 0.5, shape = 'circular', lesion_type = 'reduced', frac_pos = 0.5, both_lesion = False, mask = None, window_type = None, **kwargs):\n",
    "    lesions_parts = []\n",
    "    all_slices = []\n",
    "    all_ground_truth = []\n",
    "    all_labels = []\n",
    "    \n",
    "    participant_list = dataset['subj_idx']\n",
    "    \n",
    "    for part in participant_list: \n",
    "        print(part)\n",
    "        \n",
    "        path_data = os.path.join(working_dir, dataset['path'][part])\n",
    "        scan = nib.load(path_data)\n",
    "        struct_arr = scan.get_data().astype(np.float32)\n",
    "        struct_arr = struct_arr[10:150, :, :135]\n",
    "        slices = [struct_arr[:, :, i] for i in range(struct_arr.shape[2])]\n",
    "\n",
    "        coords = []\n",
    "\n",
    "        labels = np.zeros(len(slices))\n",
    "        num_positives = int(frac_pos * len(slices))\n",
    "        idx_pos = np.random.choice(len(slices), num_positives, replace = False)\n",
    "        \n",
    "        lesions_parts.append((part, idx_pos))\n",
    "\n",
    "        ground_truths = []\n",
    "        for i, img in enumerate(slices): \n",
    "            if i in idx_pos: \n",
    "                coords = get_coordinates(slices[i], num_lesions = num_lesions, mask = mask)\n",
    "                lesion_img, ground_truth = create_simple_lesions(slices[i], list_coords = coords, size = size, \n",
    "                                                                intensities = intensity, shape = shape, lesion_type = lesion_type, window_type = window_type)\n",
    "\n",
    "                slices[i] = lesion_img\n",
    "                labels[i] = 1\n",
    "            else: \n",
    "                if both_lesion: \n",
    "                    coords = get_coordinates(slices[i], num_lesions = kwargs['num_lesions_2'], mask = kwargs['mask_2'])\n",
    "                    lesion_img, ground_truth = create_simple_lesions(slices[i], list_coords = coords, size = kwargs['size_2'], \n",
    "                                                                    intensities = kwargs['intensity_2'], shape = kwargs['shape_2'], lesion_type = kwargs['lesion_type_2'], window_type = kwards['window_type'])\n",
    "                \n",
    "                    slices[i] = lesion_img\n",
    "                else: \n",
    "                    coords = None  \n",
    "                    ground_truth = np.zeros((slices[i].shape[0], slices[i].shape[1]))\n",
    "                \n",
    "            ground_truths.append((i, coords, ground_truth))\n",
    "        \n",
    "        all_slices.append(slices)\n",
    "        all_ground_truth.append(ground_truths)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    return lesions_parts, all_slices, all_ground_truth, all_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = training_set\n",
    "name = 'training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_ = deepcopy(slices[60])\n",
    "mask = np.zeros((slice_.shape[0], slice_.shape[1]))\n",
    "mask[:slice_.shape[0]//2, :slice_.shape[1]] = 1\n",
    "mask_inverted = (mask * -1) + 1\n",
    "plt.imshow(mask_inverted)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_group_2 = {'num_lesions_2' : 1, 'size_2' : 8, 'intensity_2' : [0.3], 'shape_2' : ['gaussian'], 'lesion_type_2' : [''], 'mask_2' : None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesions_parts, all_slices, all_ground_truth, all_labels = create_lesions_slices(working_dir = working_dir, dataset = dataset, num_lesions = 1, shape = ['window'], intensity = [0.3], lesion_type = [''], size = 8, both_lesion = False, mask = mask, window_type = scipy.signal.hamming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(all_slices[0][50])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(all_ground_truth[0][50][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(all_slices[0][52])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(all_ground_truth[0][52][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_pkl(array, data_dir, file_name):\n",
    "    with open(os.path.join(data_dir, f'{file_name}.pkl'), 'wb') as f: \n",
    "        pkl.dump(array, f)\n",
    "        \n",
    "def save_data_h5py(list_data, list_labels, data_dir, file_name): \n",
    "    all_data = np.array(reduce(operator.add, list_data))\n",
    "    all_labels = [label for labels_scan in list_labels for label in labels_scan]\n",
    "    assert len(all_data) == len(all_labels)\n",
    "    \n",
    "    h5 = h5py.File(os.path.join(data_dir, f'{file_name}.h5'), 'w')\n",
    "    h5.create_dataset('X', data=all_data, compression='gzip', compression_opts=9)\n",
    "    h5.create_dataset('y', data=all_labels, compression='gzip', compression_opts=9)\n",
    "    h5.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting at\" + time.ctime())\n",
    "start = time.time()\n",
    "\n",
    "save_data_pkl(lesions_parts, data_dir = data_dir, file_name = f'lesion_locations_slices_{name}')\n",
    "save_data_pkl(all_ground_truth, data_dir = data_dir, file_name = f'ground_truth_maps_{name}')\n",
    "save_data_pkl(all_slices, data_dir = data_dir, file_name = f'slices_parts_{name}')\n",
    "save_data_pkl(all_labels, data_dir = data_dir, file_name = f'labels_{name}')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Runtime: \" + str(datetime.timedelta(seconds = (end - start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting at\" + time.ctime())\n",
    "start = time.time()\n",
    "\n",
    "save_data_h5py(all_slices, all_labels, data_dir = data_dir, file_name = f'{name}_data')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Runtime: \" + str(datetime.timedelta(seconds = (end - start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
